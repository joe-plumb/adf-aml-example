{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "# from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n",
    "\n",
    "# Default datastore\n",
    "def_blob_store = ws.get_default_datastore() \n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster-2\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"Standard_DS3_v2\",\n",
    "                                                                min_nodes = 0, \n",
    "                                                                max_nodes = 1)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Azure Machine Learning Compute attached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses default values for PythonScriptStep construct.\n",
    "\n",
    "source_directory = '../code/scripts'\n",
    "os.mkdir(source_directory)\n",
    "print(\"Path is created\")\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/script.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "print(\"In script.py\")\n",
    "print(\"As a data scientist, this is where I use my training code.\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "\n",
    "parser.add_argument(\"--pipeline_arg\", type=str, help=\"pipeline_arg\")\n",
    "# parser.add_argument(\"--output_train\", type=str, help=\"output_train directory\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"Argument 1: %s\" % args.pipeline_arg)\n",
    "# print(\"Argument 2: %s\" % args.output_train)\n",
    "\n",
    "# if not (args.output_train is None):\n",
    "#     os.makedirs(args.output_train, exist_ok=True)\n",
    "#     print(\"%s created\" % args.output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path parameter\n",
    "data_path = DataPath(datastore=def_blob_store, path_on_datastore='sample_datapath1')\n",
    "datapath1_pipeline_param = PipelineParameter(name=\"input_datapath\", default_value=data_path)\n",
    "datapath_input = (datapath1_pipeline_param, DataPathComputeBinding(mode='moun\n",
    "\n",
    "# String Parameter\n",
    "pipeline_param = PipelineParameter(name=\"pipeline_arg\", default_value=\"default_val\")\n",
    "\n",
    "\n",
    "step1 = PythonScriptStep(name=\"script_step\",\n",
    "                         script_name=\"script.py\",\n",
    "                         arguments=[\"--pipeline_arg\", pipeline_param],\n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=source_directory,\n",
    "                         allow_reuse=True)\n",
    "print(\"Step1 created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [step1]\n",
    "print(\"Step lists created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=steps)\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_run1 = Experiment(ws, 'pipeline1-submit').submit(pipeline1, regenerate_outputs=False, \\\n",
    "#                                                           pipeline_parameters={'pipeline_arg':'hello pipeline param'})\n",
    "# print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunDetails(pipeline_run1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline1.publish(name=\"ParamPipeline\", description=\"Pipeline to test parameters\", continue_on_step_failure=True)\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "pipeline_endpoint = PipelineEndpoint.publish(workspace=ws, name=\"ParamPipelineEndpoint\",\n",
    "                                            pipeline=pipeline1, description=\"Test description Notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python36964bitfffbe1b62a1e4403aa3dc5479da7194a",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}