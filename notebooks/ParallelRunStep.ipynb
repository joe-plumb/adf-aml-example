{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitfffbe1b62a1e4403aa3dc5479da7194a",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster-2\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "# Default datastore\n",
    "def_blob_store = ws.get_default_datastore() \n",
    "# The following call GETS the Azure Blob Store associated with your workspace.\n",
    "# Note that workspaceblobstore is **the name of this store and CANNOT BE CHANGED and must be used as is** \n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))\n",
    "\n",
    "sample_csv = 'sample_data'\n",
    "\n",
    "path_on_datastore = def_blob_store.path('samples/*.csv')\n",
    "input_sample_ds = Dataset.File.from_files(path=path_on_datastore, validate=True)\n",
    "# named_sample_ds = input_sample_ds.as_named_input(sample_csv)\n",
    "\n",
    "#input_sample_ds.register(ws, 'sample_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses default values for PythonScriptStep construct.\n",
    "\n",
    "source_directory = '../code/scripts'\n",
    "script_file = 'sample_csv_script.py'\n",
    "# os.mkdir(source_directory)\n",
    "print(\"Path is created\")\n",
    "print('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $source_directory/$script_file\n",
    "# import argparse\n",
    "# import os\n",
    "\n",
    "# try:\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     # parser.add_argument(\"--arg1\", type=str, help=\"sample string argument\")\n",
    "#     parser.add_argument(\"--arg1\", type=str, help=\"sample datapath argument\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # print(\"Sample string argument  : %s\" % args.arg1)\n",
    "#     print(\"Sample datapath argument: %s\" % args.datapath)\n",
    "\n",
    "#     print(datapath_input, dir(datapath_input))\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Encountered error processing {datapath_input} :\")\n",
    "#     print(e)\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"In script.py\")\n",
    "print(\"As a data scientist, this is where I use my training code.\")\n",
    "\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "\n",
    "parser.add_argument(\"--pipeline_arg\", type=str, help=\"pipeline_arg\")\n",
    "parser.add_argument(\"--sampledata\", type=str, help=\"sample data files\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(\"Argument 1: %s\" % args.pipeline_arg)\n",
    "print(\"Argument 2: %s\" % args.sampledata)\n",
    "\n",
    "for fname in args.sampledata:\n",
    "    try:\n",
    "        with open(fname, 'r') as fin:\n",
    "            print(fin.read())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An ERROR happened: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "\n",
    "input_sample_ds.as_mount()\n",
    "\n",
    "#data_path = DataPath(datastore=def_blob_store, path_on_datastore='samples/*.csv')\n",
    "datapath1_pipeline_param = PipelineParameter(name=\"input_datapath\", default_value=input_sample_ds)\n",
    "#datapath_input = (datapath1_pipeline_param, DataPathComputeBinding(mode='mount'))\n",
    "datapath_input = input_sample_ds.as_named_input('sampledata').as_mount('/tmp/samples') #DatasetConsumptionConfig('sample_data', datapath1_pipeline_param, mode='mount')\n",
    "\n",
    "\n",
    "#datapath_input.path_on_compute()\n",
    "#string_pipeline_param = PipelineParameter(name=\"input_string\", default_value='sample_string1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "output_folder = PipelineData(name='outputs', datastore=datastore)\n",
    "\n",
    "env = Environment.get(workspace=ws, name='AzureML-Minimal') \n",
    "\n",
    "# In a real-world scenario, you'll want to shape your process per node and nodes to fit your problem domain.\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=source_directory,\n",
    "    entry_script=script_file,  # the user script to run against each input\n",
    "    mini_batch_size='1',\n",
    "    error_threshold=0,\n",
    "    output_action='append_row',\n",
    "    environment=env,\n",
    "    compute_target=compute_target, \n",
    "    node_count=1\n",
    ")\n",
    "\n",
    "parallelrunstep = ParallelRunStep(\n",
    "    name=\"sampleparallelrun\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[datapath_input],\n",
    "    output=output_folder,\n",
    "    arguments=[\"--arg1\", datapath1_pipeline_param]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [parallelrunstep]\n",
    "print(\"Step lists created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=steps)\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run1 = Experiment(ws, 'pipeline3-submit').submit(pipeline1, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  }
 ]
}